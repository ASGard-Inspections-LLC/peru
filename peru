#! /usr/bin/env python3

import collections
import distutils.dir_util
import hashlib
import json
import os
import sys

peru_file_env = {}
# TODO: "build". "deps"?
required_general_fields = {"name", "dest"}
all_general_fields = {"build"} | required_general_fields
general_fields_to_hash = {"build"}


def validate_plugin(name, required_fields, optional_fields, callback):
    assert None not in (name, callback)
    assert name not in peru_file_env, "Plugin names must be unique: " + name
    assert isinstance(required_fields, set)
    if optional_fields:
        assert isinstance(optional_fields, set)
        assert not (required_fields & optional_fields), \
            "Mandatory and optional fields must not overlap."
        all_plugin_fields = required_fields | optional_fields
    else:
        all_plugin_fields = required_fields
    overlapping_fields = all_plugin_fields & all_general_fields
    assert not overlapping_fields, \
        "Field names already taken: " + ", ".join(overlapping_fields)


def validate_rule(name, all_fields, required_fields, kwargs):
    for field in required_fields:
        assert field in kwargs, \
            "{0} requires a {1} field.".format(name, field)
    for field in kwargs:
        assert field in all_fields, \
            "{0} does not allow a {1} field.".format(name, field)


def rule_hash(kwargs, all_plugin_fields):
    # To hash this dictionary of fields, serialize it as a JSON string, and
    # take the SHA1 of that string. Dictionary key order is unspecified, so
    # "sort_keys" keeps our hash stable. Specifying separators makes the JSON
    # slightly more compact, and protects us against changes in the default.
    # "ensure_ascii" defaults to true, so specifying it just protects us from
    # changes in the default.
    hashable_kwargs = {key: val for key, val in kwargs.items()
                       if key in all_plugin_fields
                       or key in general_fields_to_hash}
    json_representation = json.dumps(hashable_kwargs, sort_keys=True,
                                     ensure_ascii=True, separators=(',', ':'))
    sha1 = hashlib.sha1()
    sha1.update(json_representation.encode("utf8"))
    return sha1.hexdigest()


def cached_files_path(key):
    return os.path.join(peru_cache_root(), "cache", key)


def has_cached_files(key):
    return os.path.isdir(cached_files_path(key))


def save_files_to_cache(key, filtered_kwargs, get_files_callback):
    print("writing cache", key)
    p = cached_files_path(key)
    os.makedirs(p)
    get_files_callback(filtered_kwargs, p)


def retrieve_files_from_cache(key, dest):
    print('retrieving cache', key)
    # TODO: Make the cache an internal git repository and let git handle all
    #       this stuff for us.
    src = cached_files_path(key)
    # shutil.copytree doesn't work if dest already exists. Use this instead.
    distutils.dir_util.copy_tree(src, dest, preserve_symlinks=True)


def peru_register(*, name=None, required_fields=None, get_files_callback=None,
                  optional_fields=None):
    plugin_name = name
    validate_plugin(plugin_name, required_fields, optional_fields,
                    get_files_callback)
    all_required_fields = required_general_fields | required_fields
    all_plugin_fields = required_fields
    if optional_fields:
        all_plugin_fields |= optional_fields
    all_fields = all_general_fields | all_plugin_fields

    def plugin_function(**kwargs):
        validate_rule(plugin_name, all_fields, all_required_fields, kwargs)
        # TODO: When we have a cache, use that as the target dir instead of the
        # final dest dir.
        rule_name = kwargs["name"]
        dest = kwargs["dest"]
        cache_key = rule_hash(kwargs, all_plugin_fields)
        print(plugin_name, rule_name)
        os.makedirs(dest, exist_ok=True)
        filtered_kwargs = {key: val for key, val in kwargs.items()
                           if key in all_plugin_fields}
        if not has_cached_files(cache_key):
            save_files_to_cache(cache_key, filtered_kwargs, get_files_callback)
        retrieve_files_from_cache(cache_key, dest)

    plugin_function.__name__ = plugin_name
    peru_file_env[plugin_name] = plugin_function


def peru_cache_root():
    # Use $PERU_CACHE_NAME if defined, otherwise use the default root path.
    return os.getenv("PERU_CACHE_NAME") or ".peru-cache"


def plugin_kwargs():
    return {
        "register": peru_register,
        "cache_root": peru_cache_root,
    }


def main():
    if not os.path.isfile("peru"):
        print("No peru file found.")
        sys.exit(1)

    # TODO: stop hardcoding this
    plugins = ["git_plugin"]
    for plugin_name in plugins:
        # fromlist just needs to be nonempty here. weird.
        plugin = __import__("plugins." + plugin_name, fromlist=["dummy"])
        plugin.peru_plugin_main(**plugin_kwargs())

    peru_file_name = os.getenv("PERU_FILE_NAME") or "peru"
    with open(peru_file_name) as peru_file:
        peru_file_code = peru_file.read()

    # TODO: Should this be an import too?
    exec(peru_file_code, dict(peru_file_env))


if __name__ == "__main__":
    main()
