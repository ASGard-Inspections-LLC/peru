#! /usr/bin/env python3

import collections
import distutils.dir_util
import hashlib
import json
import os
import subprocess
import sys
import tempfile

sys.path.append(
    os.path.join(
        os.path.dirname(os.path.realpath(__file__)),
        "third-party", "PyYAML-3.10", "lib3"))
import yaml


plugins = {}

required_general_fields = {"type", "dest"}

all_general_fields = {"build", "subdir"} | required_general_fields


def is_verbose():
    return "-v" in sys.argv or "--verbose" in sys.argv


# TODO: This function is sort of duplicated in git_plugin.py.
def verbose(*args, **kwargs):
    if is_verbose():
        print(*args, **kwargs)


def validate_plugin(name, all_plugin_fields, required_plugin_fields, callback):
    assert None not in (name, callback)
    assert name not in plugins, "Plugin names must be unique: " + name
    overlapping_fields = all_plugin_fields & all_general_fields
    assert not overlapping_fields, \
        "Field names already taken: " + ", ".join(overlapping_fields)


def validate_rule(name, all_fields, all_required_fields, rule):
    for field in all_required_fields:
        assert field in rule, \
            "{0} requires a {1} field.".format(name, field)
    for field in rule:
        assert field in all_fields, \
            "{0} does not allow a {1} field.".format(name, field)


def rule_hash(rule, all_plugin_fields):
    # TODO: Are there fields (maybe "dest") that should be excluded from the
    # cache key?

    # To hash this dictionary of fields, serialize it as a JSON string, and
    # take the SHA1 of that string. Dictionary key order is unspecified, so
    # "sort_keys" keeps our hash stable. Specifying separators makes the JSON
    # slightly more compact, and protects us against changes in the default.
    # "ensure_ascii" defaults to true, so specifying it just protects us from
    # changes in the default.
    json_representation = json.dumps(rule, sort_keys=True, ensure_ascii=True,
                                     separators=(',', ':'))
    sha1 = hashlib.sha1()
    sha1.update(json_representation.encode("utf8"))
    return sha1.hexdigest()


def cached_files_path(cache_key):
    return os.path.join(peru_cache_root(), "cache", cache_key)


def has_cached_files(cache_key):
    return os.path.isdir(cached_files_path(cache_key))


def save_files_to_cache(cache_key, filtered_fields, build, subdir,
                        get_files_callback):
    verbose("writing cache", cache_key)

    # Invoke the appropriate plugin (e.g. git_plugin) to dump all the specified
    # files into a temporary directory.
    os.makedirs("/tmp/peru", exist_ok=True)
    temp_path = tempfile.mkdtemp(dir="/tmp/peru")
    get_files_callback(filtered_fields, temp_path)

    # If the user supplied a build command, run that command on the files.
    if build:
        verbose('building "{0}"...'.format(build))
        subprocess.check_call(build, shell=True, cwd=temp_path)

    # If the user supplied a subdir, set the copy src to that.
    src = temp_path
    if subdir:
        src = os.path.join(temp_path, subdir)

    # Finally, copy all the resulting files into the cache.
    distutils.dir_util.copy_tree(src, cached_files_path(cache_key),
                                 preserve_symlinks=True)


def retrieve_files_from_cache(cache_key, dest):
    verbose('retrieving cache', cache_key)
    # TODO: Make the cache an internal git repository and let git handle all
    #       this stuff for us.
    src = cached_files_path(cache_key)
    # shutil.copytree doesn't work if dest already exists. Use this instead.
    distutils.dir_util.copy_tree(src, dest, preserve_symlinks=True)


def peru_register(*, name=None, required_fields=None, get_files_callback=None,
                  optional_fields=None):
    plugin_name = name
    all_required_fields = required_general_fields | required_fields
    all_plugin_fields = required_fields
    if optional_fields:
        all_plugin_fields |= optional_fields
    all_fields = all_general_fields | all_plugin_fields
    validate_plugin(plugin_name, all_plugin_fields, required_fields,
                    get_files_callback)

    def plugin_function(rule_name, rule):
        validate_rule(plugin_name, all_fields, all_required_fields, rule)
        dest = rule["dest"]
        build = rule.get("build")
        subdir = rule.get("subdir")
        cache_key = rule_hash(rule, all_plugin_fields)
        verbose(plugin_name, rule_name)
        filtered_fields = {key: val for key, val in rule.items()
                           if key in all_plugin_fields}
        if not has_cached_files(cache_key):
            save_files_to_cache(cache_key, filtered_fields, build, subdir,
                                get_files_callback)
        retrieve_files_from_cache(cache_key, dest)

    plugin_function.__name__ = plugin_name
    plugins[plugin_name] = plugin_function


def peru_cache_root():
    # Use $PERU_CACHE_NAME if defined, otherwise use the default root path.
    return os.getenv("PERU_CACHE_NAME") or ".peru-cache"


def plugin_kwargs():
    return {
        "register": peru_register,
        "cache_root": peru_cache_root,
        "verbose": is_verbose(),
    }


def execute_peru_config(config):
    for section_name, section in config.items():
        if section_name.split()[0] == "rule":
            rule_name = section_name.split()[1]
            assert "type" in section, "All rules must specify a type."
            type_name = section["type"]
            assert type_name in plugins, \
                'Type "{0}" is not defined.'.format(type_name)
            plugins[type_name](rule_name, section)


def main():
    # TODO: stop hardcoding this
    plugins = ["git_plugin", "local_plugin"]
    for plugin_name in plugins:
        # fromlist just needs to be nonempty here. weird.
        plugin = __import__("plugins." + plugin_name, fromlist=["dummy"])
        plugin.peru_plugin_main(**plugin_kwargs())

    peru_file_name = os.getenv("PERU_FILE_NAME") or "peru"
    if not os.path.isfile(peru_file_name):
        print("No peru file found.")
        sys.exit(1)

    with open(peru_file_name) as peru_file:
        config = yaml.safe_load(peru_file)

    execute_peru_config(config)


if __name__ == "__main__":
    main()
