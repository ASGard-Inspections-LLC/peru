#! /usr/bin/env python3

import collections
import distutils.dir_util
import hashlib
import json
import os
import subprocess
import sys
import tempfile

peru_file_env = {}
required_general_fields = {"name", "dest"}
all_general_fields = {"build", "subdir"} | required_general_fields
general_fields_to_hash = {"build", "subdir"}


def validate_plugin(name, all_plugin_fields, required_plugin_fields, callback):
    assert None not in (name, callback)
    assert name not in peru_file_env, "Plugin names must be unique: " + name
    overlapping_fields = all_plugin_fields & all_general_fields
    assert not overlapping_fields, \
        "Field names already taken: " + ", ".join(overlapping_fields)


def validate_rule(name, all_fields, all_required_fields, kwargs):
    for field in all_required_fields:
        assert field in kwargs, \
            "{0} requires a {1} field.".format(name, field)
    for field in kwargs:
        assert field in all_fields, \
            "{0} does not allow a {1} field.".format(name, field)


def rule_hash(kwargs, all_plugin_fields):
    # To hash this dictionary of fields, serialize it as a JSON string, and
    # take the SHA1 of that string. Dictionary key order is unspecified, so
    # "sort_keys" keeps our hash stable. Specifying separators makes the JSON
    # slightly more compact, and protects us against changes in the default.
    # "ensure_ascii" defaults to true, so specifying it just protects us from
    # changes in the default.
    hashable_kwargs = {key: val for key, val in kwargs.items()
                       if key in all_plugin_fields
                       or key in general_fields_to_hash}
    json_representation = json.dumps(hashable_kwargs, sort_keys=True,
                                     ensure_ascii=True, separators=(',', ':'))
    sha1 = hashlib.sha1()
    sha1.update(json_representation.encode("utf8"))
    return sha1.hexdigest()


def cached_files_path(cache_key):
    return os.path.join(peru_cache_root(), "cache", cache_key)


def has_cached_files(cache_key):
    return os.path.isdir(cached_files_path(cache_key))


def save_files_to_cache(cache_key, filtered_kwargs, build, subdir,
                        get_files_callback):
    print("writing cache", cache_key)

    # Invoke the appropriate plugin (e.g. git_plugin) to dump all the specified
    # files into a temporary directory.
    os.makedirs("/tmp/peru", exist_ok=True)
    temp_path = tempfile.mkdtemp(dir="/tmp/peru")
    get_files_callback(filtered_kwargs, temp_path)

    # If the user supplied a build command, run that command on the files.
    if build:
        print('building "{0}"...'.format(build))
        subprocess.check_call(build, shell=True, cwd=temp_path)

    # If the user supplied a subdir, set the copy src to that.
    src = temp_path
    if subdir:
        src = os.path.join(temp_path, subdir)

    # Finally, copy all the resulting files into the cache.
    distutils.dir_util.copy_tree(src, cached_files_path(cache_key),
                                 preserve_symlinks=True)


def retrieve_files_from_cache(cache_key, dest):
    print('retrieving cache', cache_key)
    # TODO: Make the cache an internal git repository and let git handle all
    #       this stuff for us.
    src = cached_files_path(cache_key)
    # shutil.copytree doesn't work if dest already exists. Use this instead.
    distutils.dir_util.copy_tree(src, dest, preserve_symlinks=True)


def peru_register(*, name=None, required_fields=None, get_files_callback=None,
                  optional_fields=None):
    plugin_name = name
    all_required_fields = required_general_fields | required_fields
    all_plugin_fields = required_fields
    if optional_fields:
        all_plugin_fields |= optional_fields
    all_fields = all_general_fields | all_plugin_fields
    validate_plugin(plugin_name, all_plugin_fields, required_fields,
                    get_files_callback)

    def plugin_function(**kwargs):
        validate_rule(plugin_name, all_fields, all_required_fields, kwargs)
        # TODO: When we have a cache, use that as the target dir instead of the
        # final dest dir.
        rule_name = kwargs["name"]
        dest = kwargs["dest"]
        build = kwargs.get("build")
        subdir = kwargs.get("subdir")
        cache_key = rule_hash(kwargs, all_plugin_fields)
        print(plugin_name, rule_name)
        filtered_kwargs = {key: val for key, val in kwargs.items()
                           if key in all_plugin_fields}
        if not has_cached_files(cache_key):
            save_files_to_cache(cache_key, filtered_kwargs, build, subdir,
                                get_files_callback)
        retrieve_files_from_cache(cache_key, dest)

    plugin_function.__name__ = plugin_name
    peru_file_env[plugin_name] = plugin_function


def peru_cache_root():
    # Use $PERU_CACHE_NAME if defined, otherwise use the default root path.
    return os.getenv("PERU_CACHE_NAME") or ".peru-cache"


def plugin_kwargs():
    return {
        "register": peru_register,
        "cache_root": peru_cache_root,
    }


def main():
    if not os.path.isfile("peru"):
        print("No peru file found.")
        sys.exit(1)

    # TODO: stop hardcoding this
    plugins = ["git_plugin"]
    for plugin_name in plugins:
        # fromlist just needs to be nonempty here. weird.
        plugin = __import__("plugins." + plugin_name, fromlist=["dummy"])
        plugin.peru_plugin_main(**plugin_kwargs())

    peru_file_name = os.getenv("PERU_FILE_NAME") or "peru"
    with open(peru_file_name) as peru_file:
        peru_file_code = peru_file.read()

    # TODO: Should this be an import too?
    exec(peru_file_code, dict(peru_file_env))


if __name__ == "__main__":
    main()
