#! /usr/bin/env python3

import collections
import hashlib
import json
import os
import sys

peru_file_env = {}
# TODO: "build". "deps"?
general_fields = ("name", "dest")


def validate_plugin(name, fields, callback):
    assert None not in (name, fields, callback)
    assert name not in peru_file_env, "Plugin names must be unique: " + name
    overlapping_fields = set(fields) & set(general_fields)
    assert not overlapping_fields, \
        "Field names already taken: " + ", ".join(overlapping_fields)


def validate_rule(name, fields, kwargs):
    for field in general_fields + fields:
        assert field in kwargs, \
            "{0} requires a {1} field.".format(name, field)
    for field in kwargs:
        assert field in general_fields + fields, \
            "{0} does not allow a {1} field.".format(name, field)


# TODO: Some general fields (like "build") should be included in
# this hash as well.
def rule_hash(filtered_kwargs):
    # To hash this dictionary of fields, serialize it as a JSON string, and
    # take the SHA1 of that string. Dictionary key order is unspecified, so
    # "sort_keys" keeps our hash stable. Specifying separators makes the JSON
    # slightly more compact, and protects us against changes in the default.
    # "ensure_ascii" defaults to true, so specifying it just protects us from
    # changes in the default.
    json_representation = json.dumps(filtered_kwargs, sort_keys=True,
                                     ensure_ascii=True, separators=(',', ':'))
    sha1 = hashlib.sha1()
    sha1.update(json_representation.encode("utf8"))
    return sha1.hexdigest()


def cached_files_path(key):
    return os.path.join(peru_cache_root(), "cache", key)


def has_cached_files(key):
    return os.path.isdir(cached_files_path(key))


def peru_register(*, name=None, fields=None, get_files_callback=None):
    validate_plugin(name, fields, get_files_callback)
    def plugin_function(**kwargs):
        print(name)
        validate_rule(name, fields, kwargs)
        # TODO: When we have a cache, use that as the target dir instead of the
        # final dest dir.
        dest = kwargs["dest"]
        os.makedirs(dest, exist_ok=True)
        filtered_kwargs = {key: val for key, val in kwargs.items()
                           if key in fields}
        cache_key = rule_hash(filtered_kwargs)
        if not has_cached_files(cache_key):
            print(cache_key + " is not in cache!")
        # TODO: Get from cache here, instead of getting
        #       directly from the plugin.
        get_files_callback(filtered_kwargs, dest)
    plugin_function.__name__ = name
    peru_file_env[name] = plugin_function


def peru_cache_root():
    # Use $PERU_CACHE_NAME if defined, otherwise use the default root path.
    return os.getenv("PERU_CACHE_NAME") or ".peru-cache"


def plugin_kwargs():
    return {
        "register": peru_register,
        "cache_root": peru_cache_root,
    }


def main():
    if not os.path.isfile("peru"):
        print("No peru file found.")
        sys.exit(1)

    # TODO: stop hardcoding this
    plugins = ["git_plugin"]
    for plugin_name in plugins:
        # fromlist just needs to be nonempty here. weird.
        plugin = __import__("plugins." + plugin_name, fromlist=["dummy"])
        plugin.peru_plugin_main(**plugin_kwargs())

    peru_file_name = os.getenv("PERU_FILE_NAME") or "peru"
    with open(peru_file_name) as peru_file:
        peru_file_code = peru_file.read()

    # TODO: Should this be an import too?
    exec(peru_file_code, dict(peru_file_env))


if __name__ == "__main__":
    main()
